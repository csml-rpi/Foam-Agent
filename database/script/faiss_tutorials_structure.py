import os
import re
import argparse
from pathlib import Path

from langchain_community.vectorstores import FAISS
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_core.documents import Document

def extract_field(field_name: str, text: str) -> str:
    """
    ä»æ–‡æœ¬ä¸­æå–æŒ‡å®šå­—æ®µçš„å†…å®¹ã€‚
    ä¾‹å¦‚ï¼šfield_name="case name"ï¼ŒtextåŒ…å«"case name: cavity"
    """
    match = re.search(fr"{field_name}:\s*(.*)", text)
    return match.group(1).strip() if match else "Unknown"

def tokenize(text: str) -> str:
    """
    å¯¹æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼š
    1. ä¸‹åˆ’çº¿è½¬ç©ºæ ¼
    2. é©¼å³°åˆ†è¯
    3. è½¬å°å†™
    """
    text = text.replace('_', ' ')
    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)
    return text.lower()

def main():
    """
    ä¸»æµç¨‹ï¼š
    1. è§£æå‚æ•°
    2. è¯»å–æ•™ç¨‹ç»“æ„åŸå§‹æ•°æ®
    3. è§£ææ¯ä¸ªæ¡ˆä¾‹çš„ç»“æ„å’Œå…ƒæ•°æ®
    4. ç”Ÿæˆå‘é‡å¹¶å­˜å…¥FAISS
    5. ä¿å­˜ç´¢å¼•
    """
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="Process OpenFOAM case data and store embeddings in FAISS."
    )
    parser.add_argument(
        "--database_path",
        type=str,
        default=Path(__file__).resolve().parent.parent,
        help="Path to the database directory (default: '../../')",
    )
    args = parser.parse_args()
    database_path = args.database_path
    print(f"ğŸ“‚ æ•™ç¨‹ç»“æ„æ•°æ®åº“è·¯å¾„: {database_path}")

    # 2. è¯»å–è¾“å…¥æ–‡ä»¶
    database_allrun_path = os.path.join(database_path, "raw/openfoam_tutorials_structure.txt")
    print(f"ğŸ“„ è¯»å–æ–‡ä»¶: {database_allrun_path}")
    if not os.path.exists(database_allrun_path):
        raise FileNotFoundError(f"File not found: {database_allrun_path}")

    with open(database_allrun_path, "r", encoding="utf-8") as file:
        file_content = file.read()
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {len(file_content)} å­—ç¬¦")
    print(f"ğŸ“‹ æ–‡ä»¶å†…å®¹é¢„è§ˆ: {file_content[:200]}...")

    # 3. ç”¨æ­£åˆ™æå–æ¯ä¸ª<case_begin>...</case_end>ç‰‡æ®µ
    pattern = re.compile(r"<case_begin>(.*?)</case_end>", re.DOTALL)
    matches = pattern.findall(file_content)
    print(f"ğŸ” æ‰¾åˆ° {len(matches)} ä¸ªæ¡ˆä¾‹ç‰‡æ®µ")
    if not matches:
        raise ValueError("No cases found in the input file. Please check the file content.")

    documents = []
    for i, match in enumerate(matches):
        print(f"\nğŸ“‹ å¤„ç†æ¡ˆä¾‹ {i+1}/{len(matches)}")
        full_content = match.strip()  # å­˜å‚¨å®Œæ•´çš„æ¡ˆä¾‹å†…å®¹

        # æå–<index>å†…å®¹
        index_match = re.search(r"<index>(.*?)</index>", match, re.DOTALL)
        if not index_match:
            print("  âŒ æœªæ‰¾åˆ°<index>æ ‡ç­¾ï¼Œè·³è¿‡")
            continue
        index_content = index_match.group(1).strip()  # åªæå–<index>å†…éƒ¨å†…å®¹
        print(f"  ğŸ“ indexå†…å®¹é¢„è§ˆ: {index_content[:100]}...")

        # æå–å…ƒæ•°æ®å­—æ®µ
        case_name = extract_field("case name", index_content)
        case_domain = extract_field("case domain", index_content)
        case_category = extract_field("case category", index_content)
        case_solver = extract_field("case solver", index_content)
        print(f"  ğŸ·ï¸ åç§°: {case_name}, åŸŸ: {case_domain}, ç±»åˆ«: {case_category}, æ±‚è§£å™¨: {case_solver}")

        # æå–ç›®å½•ç»“æ„
        dir_match = re.search(r"<directory_structure>([\s\S]*?)</directory_structure>", full_content)
        case_directory_structure = dir_match.group(1) if dir_match else "Unknown"
        print(f"  ğŸ“‚ ç›®å½•ç»“æ„é•¿åº¦: {len(case_directory_structure)} å­—ç¬¦")
        print(f"  ğŸ“‚ ç›®å½•ç»“æ„é¢„è§ˆ: {case_directory_structure[:100]}...")

        # åˆ›å»ºDocumentå¯¹è±¡
        doc = Document(
            page_content=tokenize(index_content),  # åªç”¨<index>å†…å®¹åšå‘é‡åŒ–
            metadata={
                "full_content": full_content,  # å­˜å®Œæ•´å†…å®¹
                "case_name": case_name,
                "case_domain": case_domain,
                "case_category": case_category,
                "case_solver": case_solver,
                'dir_structure': case_directory_structure
            }
        )
        documents.append(doc)
        print(f"  âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸ")

    print(f"\nğŸ“Š å…±åˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹ç”Ÿæˆå‘é‡åµŒå…¥...")

    # 4. è®¡ç®—åµŒå…¥å¹¶å­˜å…¥FAISS
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
    vectordb = FAISS.from_documents(documents, embedding_model)
    print(f"âœ… å‘é‡åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œæ•°æ®åº“å¤§å°: {vectordb.index.ntotal}")

    # 5. ä¿å­˜FAISSç´¢å¼•
    persist_directory = os.path.join(database_path, "faiss/openfoam_tutorials_structure")
    print(f"ğŸ’¾ ä¿å­˜FAISSç´¢å¼•åˆ°: {persist_directory}")
    vectordb.save_local(persist_directory)
    print(f"ğŸ‰ {len(documents)} cases indexed successfully with metadata! Saved at: {persist_directory}")

if __name__ == "__main__":
    main()
    
